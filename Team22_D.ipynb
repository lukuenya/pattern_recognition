{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Seraina Kytili 9728\n",
        "---\n",
        "Matthieu Ndumbi Lukuenya 9217\n",
        "---\n"
      ],
      "metadata": {
        "id": "4jrxUgME-4uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "A_x5rIHdAj_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SIG-yw4KRdmz"
      },
      "outputs": [],
      "source": [
        "# Imports.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "kEBlOJv4Ausx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing.\n",
        "# Insert the dataset into a Dataframe called movies_df\n",
        "df = pd.read_csv(\"/datasetC.csv\", header=None)\n",
        "final_x_test = pd.read_csv(\"/datasetCTest.csv\", header=None)\n",
        "\n",
        "# Separate into X (features) and y (label)\n",
        "X = df.iloc[:, :-1] # Select all columns except the last one\n",
        "y = df.iloc[:, -1] # Select the last column\n",
        "\n",
        "# Step 1: Data Splitting into train, validation and test dataset\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Get the shape of the dataset\n",
        "print(df.shape)\n",
        "print(final_x_test.shape)\n",
        "\n",
        "# Get min, max value of the dataset\n",
        "minimum_value = df.min().min()\n",
        "maximum_value = df.max().max()\n",
        "\n",
        "print(\"Minimum Value of Train dataset:\", minimum_value)\n",
        "print(\"Maximum Value of Train dataset:\", maximum_value)\n",
        "\n",
        "# Get min, max value of the datasetC_test\n",
        "minimum_value_test = final_x_test.min().min()\n",
        "maximum_value_test = final_x_test.max().max()\n",
        "\n",
        "print(\"Minimum Value of Test dataset:\", minimum_value_test)\n",
        "print(\"Maximum Value of Test dataset:\", maximum_value_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF37FxBNRomW",
        "outputId": "d9712fdc-0d70-451e-fecb-b3c97bfa82c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 401)\n",
            "(1000, 400)\n",
            "Minimum Value of Train dataset: -4.4854\n",
            "Maximum Value of Train dataset: 5.1581\n",
            "Minimum Value of Test dataset: -4.2736\n",
            "Maximum Value of Test dataset: 4.7749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values"
      ],
      "metadata": {
        "id": "a8Ey-5o4A7-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check by visualization if there are missing values\n",
        "import missingno as msno\n",
        "msno.matrix(df)\n",
        "\n",
        "# Double - Check by calculations if there are missing values\n",
        "# Check if the missing values across the matrix are equal to 0\n",
        "# Print the result of all raws\n",
        "print(df.isnull().sum().sum())\n",
        "print(final_x_test.isnull().sum().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "Ol4Ruh5AZ4Y0",
        "outputId": "edc72ba2-b4dd-4a9b-e3a2-5076d9d4c516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balanced labels"
      ],
      "metadata": {
        "id": "6ytXqo2OB-kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the labels are balanced.\n",
        "\n",
        "# Plot the distribution of labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(y=y)\n",
        "plt.title('Distribution of Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "nDG7blVpeJM0",
        "outputId": "3160bec4-3db5-47c0-c58c-cec9f35f25e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "xleqt-LgCHjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate Model"
      ],
      "metadata": {
        "id": "DXeBZJvgM9Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate a model using grid search.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The scikit-learn model (e.g., RandomForestClassifier, XGBClassifier).\n",
        "    - param_grid: The parameter grid for grid search.\n",
        "    - X_train, y_train: Training set.\n",
        "    - X_val, y_val: Validation set.\n",
        "    - X_test, y_test: Test set.\n",
        "\n",
        "    Returns:\n",
        "    grid_result\n",
        "    \"\"\"\n",
        "    # Step 1: Grid Search with Cross-Validation on Training Set\n",
        "    scorer = make_scorer(accuracy_score)\n",
        "    #scorer = make_scorer(f1_score, average='weighted')\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=5)\n",
        "    grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Step 2: Print the best parameters and best score\n",
        "    print(\"Best Parameters: \", grid_result.best_params_)\n",
        "    print(\"Best Accuracy on Training Set: \", grid_result.best_score_)\n",
        "\n",
        "    # Step 3: Evaluate on the Validation Set with the Best Model\n",
        "    best_model = grid_result.best_estimator_\n",
        "    val_predictions = best_model.predict(X_val)\n",
        "\n",
        "    # Evaluate and print metrics on the validation set\n",
        "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "    print(\"Accuracy on Validation Set with the best model: \", val_accuracy)\n",
        "\n",
        "    # Step 4: Evaluate on the Test Set with the Best Model\n",
        "    test_predictions = best_model.predict(X_test)\n",
        "\n",
        "    # Evaluate and print metrics on the test set\n",
        "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "    print(\"Accuracy on Test Set with the best model: \", test_accuracy)\n",
        "\n",
        "    return grid_result\n"
      ],
      "metadata": {
        "id": "nfDeH1Fxcfga"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display GridSearch Results"
      ],
      "metadata": {
        "id": "iottM4RhNIIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_grid_search_results(grid_result):\n",
        "    \"\"\"\n",
        "    Display and sort the results of a grid search.\n",
        "\n",
        "    Parameters:\n",
        "    - grid_result: The result from the scikit-learn GridSearchCV.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Print Scores from Grid Search\n",
        "    cv_results = pd.DataFrame(grid_result.cv_results_)\n",
        "    print(\"\\nAll Scores from Grid Search:\")\n",
        "    print(cv_results[['params', 'mean_test_score']])\n",
        "\n",
        "    # Sort the DataFrame by mean_test_score in descending order\n",
        "    cv_results_sorted = cv_results.sort_values(by='mean_test_score', ascending=False)\n",
        "\n",
        "    # Display all values without truncation\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    print(\"\\nHyperparameters sorted by Mean Test Score in Descending Order:\")\n",
        "    print(cv_results_sorted[['params', 'mean_test_score']])\n",
        "\n",
        "    # Reset display options to default\n",
        "    pd.reset_option('display.max_rows')\n",
        "    pd.reset_option('display.max_colwidth')"
      ],
      "metadata": {
        "id": "fFJgadbvbXvn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "T0fJ9y2bNOeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "GQ99fGh2NwDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NAIVE BAYES CLASSIFIER\n",
        "\n",
        "# Define the model (No hyperparameters for GaussianNB)\n",
        "model_nb = GaussianNB()\n",
        "\n",
        "# Train and evaluate the Naive Bayes Classifier\n",
        "grid_result_nb = train_and_evaluate_model(model_nb, {}, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_nb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBtMWAR2IaK",
        "outputId": "9440d0a4-1e87-43b9-b584-114afdd4fde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {}\n",
            "Best Accuracy on Training Set:  0.7939999999999999\n",
            "Accuracy on Validation Set with the best model:  0.7746666666666666\n",
            "Accuracy on Test Set with the best model:  0.8173333333333334\n",
            "\n",
            "All Scores from Grid Search:\n",
            "  params  mean_test_score\n",
            "0     {}            0.794\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "  params  mean_test_score\n",
            "0     {}            0.794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "RUhqaBquNsOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGISTIC REGRESSION\n",
        "\n",
        "# Define the model and parameter grid for grid search\n",
        "model_lr = LogisticRegression()\n",
        "param_grid_lr = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],      # Regularization parameter\n",
        "    'solver': ['liblinear', 'lbfgs'],    # Optimization algorithm\n",
        "    'max_iter': [100, 200, 300]           # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Train and evaluate the Logistic Regression using grid search\n",
        "grid_result_lr = train_and_evaluate_model(model_lr, param_grid_lr, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVfa6Ah-2EtL",
        "outputId": "7f7cbd68-077e-4f6f-b050-58169afffb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'C': 0.01, 'max_iter': 100, 'solver': 'lbfgs'}\n",
            "Best Accuracy on Training Set:  0.8182857142857142\n",
            "Accuracy on Validation Set with the best model:  0.7813333333333333\n",
            "Accuracy on Test Set with the best model:  0.8146666666666667\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                               params  mean_test_score\n",
            "0   {'C': 0.001, 'max_iter': 100, 'solver': 'libli...         0.812000\n",
            "1    {'C': 0.001, 'max_iter': 100, 'solver': 'lbfgs'}         0.816857\n",
            "2   {'C': 0.001, 'max_iter': 200, 'solver': 'libli...         0.812000\n",
            "3    {'C': 0.001, 'max_iter': 200, 'solver': 'lbfgs'}         0.816857\n",
            "4   {'C': 0.001, 'max_iter': 300, 'solver': 'libli...         0.812000\n",
            "5    {'C': 0.001, 'max_iter': 300, 'solver': 'lbfgs'}         0.816857\n",
            "6   {'C': 0.01, 'max_iter': 100, 'solver': 'liblin...         0.817143\n",
            "7     {'C': 0.01, 'max_iter': 100, 'solver': 'lbfgs'}         0.818286\n",
            "8   {'C': 0.01, 'max_iter': 200, 'solver': 'liblin...         0.817143\n",
            "9     {'C': 0.01, 'max_iter': 200, 'solver': 'lbfgs'}         0.818286\n",
            "10  {'C': 0.01, 'max_iter': 300, 'solver': 'liblin...         0.817143\n",
            "11    {'C': 0.01, 'max_iter': 300, 'solver': 'lbfgs'}         0.818286\n",
            "12  {'C': 0.1, 'max_iter': 100, 'solver': 'libline...         0.792000\n",
            "13     {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}         0.770571\n",
            "14  {'C': 0.1, 'max_iter': 200, 'solver': 'libline...         0.792000\n",
            "15     {'C': 0.1, 'max_iter': 200, 'solver': 'lbfgs'}         0.770857\n",
            "16  {'C': 0.1, 'max_iter': 300, 'solver': 'libline...         0.792000\n",
            "17     {'C': 0.1, 'max_iter': 300, 'solver': 'lbfgs'}         0.770857\n",
            "18   {'C': 1, 'max_iter': 100, 'solver': 'liblinear'}         0.768000\n",
            "19       {'C': 1, 'max_iter': 100, 'solver': 'lbfgs'}         0.734857\n",
            "20   {'C': 1, 'max_iter': 200, 'solver': 'liblinear'}         0.768000\n",
            "21       {'C': 1, 'max_iter': 200, 'solver': 'lbfgs'}         0.737143\n",
            "22   {'C': 1, 'max_iter': 300, 'solver': 'liblinear'}         0.768000\n",
            "23       {'C': 1, 'max_iter': 300, 'solver': 'lbfgs'}         0.736857\n",
            "24  {'C': 10, 'max_iter': 100, 'solver': 'liblinear'}         0.746286\n",
            "25      {'C': 10, 'max_iter': 100, 'solver': 'lbfgs'}         0.722571\n",
            "26  {'C': 10, 'max_iter': 200, 'solver': 'liblinear'}         0.746286\n",
            "27      {'C': 10, 'max_iter': 200, 'solver': 'lbfgs'}         0.723714\n",
            "28  {'C': 10, 'max_iter': 300, 'solver': 'liblinear'}         0.746286\n",
            "29      {'C': 10, 'max_iter': 300, 'solver': 'lbfgs'}         0.723429\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                                  params  mean_test_score\n",
            "7        {'C': 0.01, 'max_iter': 100, 'solver': 'lbfgs'}         0.818286\n",
            "9        {'C': 0.01, 'max_iter': 200, 'solver': 'lbfgs'}         0.818286\n",
            "11       {'C': 0.01, 'max_iter': 300, 'solver': 'lbfgs'}         0.818286\n",
            "6    {'C': 0.01, 'max_iter': 100, 'solver': 'liblinear'}         0.817143\n",
            "8    {'C': 0.01, 'max_iter': 200, 'solver': 'liblinear'}         0.817143\n",
            "10   {'C': 0.01, 'max_iter': 300, 'solver': 'liblinear'}         0.817143\n",
            "3       {'C': 0.001, 'max_iter': 200, 'solver': 'lbfgs'}         0.816857\n",
            "5       {'C': 0.001, 'max_iter': 300, 'solver': 'lbfgs'}         0.816857\n",
            "1       {'C': 0.001, 'max_iter': 100, 'solver': 'lbfgs'}         0.816857\n",
            "0   {'C': 0.001, 'max_iter': 100, 'solver': 'liblinear'}         0.812000\n",
            "2   {'C': 0.001, 'max_iter': 200, 'solver': 'liblinear'}         0.812000\n",
            "4   {'C': 0.001, 'max_iter': 300, 'solver': 'liblinear'}         0.812000\n",
            "16    {'C': 0.1, 'max_iter': 300, 'solver': 'liblinear'}         0.792000\n",
            "12    {'C': 0.1, 'max_iter': 100, 'solver': 'liblinear'}         0.792000\n",
            "14    {'C': 0.1, 'max_iter': 200, 'solver': 'liblinear'}         0.792000\n",
            "17        {'C': 0.1, 'max_iter': 300, 'solver': 'lbfgs'}         0.770857\n",
            "15        {'C': 0.1, 'max_iter': 200, 'solver': 'lbfgs'}         0.770857\n",
            "13        {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}         0.770571\n",
            "18      {'C': 1, 'max_iter': 100, 'solver': 'liblinear'}         0.768000\n",
            "20      {'C': 1, 'max_iter': 200, 'solver': 'liblinear'}         0.768000\n",
            "22      {'C': 1, 'max_iter': 300, 'solver': 'liblinear'}         0.768000\n",
            "24     {'C': 10, 'max_iter': 100, 'solver': 'liblinear'}         0.746286\n",
            "26     {'C': 10, 'max_iter': 200, 'solver': 'liblinear'}         0.746286\n",
            "28     {'C': 10, 'max_iter': 300, 'solver': 'liblinear'}         0.746286\n",
            "21          {'C': 1, 'max_iter': 200, 'solver': 'lbfgs'}         0.737143\n",
            "23          {'C': 1, 'max_iter': 300, 'solver': 'lbfgs'}         0.736857\n",
            "19          {'C': 1, 'max_iter': 100, 'solver': 'lbfgs'}         0.734857\n",
            "27         {'C': 10, 'max_iter': 200, 'solver': 'lbfgs'}         0.723714\n",
            "29         {'C': 10, 'max_iter': 300, 'solver': 'lbfgs'}         0.723429\n",
            "25         {'C': 10, 'max_iter': 100, 'solver': 'lbfgs'}         0.722571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "E9Qj4kY7NoUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-NEAREST NEIGHBORS (KNN) CLASSIFIER\n",
        "\n",
        "# Define the model and parameter grid for grid search\n",
        "model_knn = KNeighborsClassifier()\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],          # Number of neighbors\n",
        "    'weights': ['uniform', 'distance'],  # Weighting strategy\n",
        "    'p': [1, 2]                           # Power parameter for Minkowski distance\n",
        "}\n",
        "\n",
        "# Train and evaluate the KNN using grid search\n",
        "grid_result_knn = train_and_evaluate_model(model_knn, param_grid_knn, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_knn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvqQh-c51bwP",
        "outputId": "b8b015ad-916e-4d85-ab09-9e0cf41e3646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n",
            "Best Accuracy on Training Set:  0.7417142857142857\n",
            "Accuracy on Validation Set with the best model:  0.7373333333333333\n",
            "Accuracy on Test Set with the best model:  0.7546666666666667\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                               params  mean_test_score\n",
            "0    {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}         0.643143\n",
            "1   {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}         0.658857\n",
            "2    {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}         0.665429\n",
            "3   {'n_neighbors': 3, 'p': 2, 'weights': 'distance'}         0.682857\n",
            "4    {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}         0.682000\n",
            "5   {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}         0.699429\n",
            "6    {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}         0.699429\n",
            "7   {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}         0.718000\n",
            "8    {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}         0.698571\n",
            "9   {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}         0.715143\n",
            "10   {'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}         0.724571\n",
            "11  {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}         0.734571\n",
            "12   {'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}         0.708857\n",
            "13  {'n_neighbors': 9, 'p': 1, 'weights': 'distance'}         0.720000\n",
            "14   {'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}         0.726857\n",
            "15  {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}         0.741714\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                               params  mean_test_score\n",
            "15  {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}         0.741714\n",
            "11  {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}         0.734571\n",
            "14   {'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}         0.726857\n",
            "10   {'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}         0.724571\n",
            "13  {'n_neighbors': 9, 'p': 1, 'weights': 'distance'}         0.720000\n",
            "7   {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}         0.718000\n",
            "9   {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}         0.715143\n",
            "12   {'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}         0.708857\n",
            "6    {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}         0.699429\n",
            "5   {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}         0.699429\n",
            "8    {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}         0.698571\n",
            "3   {'n_neighbors': 3, 'p': 2, 'weights': 'distance'}         0.682857\n",
            "4    {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}         0.682000\n",
            "2    {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}         0.665429\n",
            "1   {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}         0.658857\n",
            "0    {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}         0.643143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-NEAREST NEIGHBORS (KNN) CLASSIFIER\n",
        "\n",
        "# Define the model\n",
        "model_knn = KNeighborsClassifier(n_neighbors=23, weights='distance', p=2)\n",
        "\n",
        "# Train and evaluate the KNN using grid search\n",
        "grid_result_knn = train_and_evaluate_model(model_knn, param_grid_knn, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_knn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAxFhKV_gBnY",
        "outputId": "1bef3321-c028-4bb4-fc92-3b2a7fa53c3e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'n_neighbors': 23, 'p': 2, 'weights': 'distance'}\n",
            "Best Accuracy on Training Set:  0.7545714285714287\n",
            "Accuracy on Validation Set with the best model:  0.744\n",
            "Accuracy on Test Set with the best model:  0.7533333333333333\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                              params  mean_test_score\n",
            "0  {'n_neighbors': 23, 'p': 2, 'weights': 'uniform'}         0.742571\n",
            "1  {'n_neighbors': 23, 'p': 2, 'weights': 'distan...         0.754571\n",
            "2  {'n_neighbors': 23, 'p': 3, 'weights': 'uniform'}         0.735143\n",
            "3  {'n_neighbors': 23, 'p': 3, 'weights': 'distan...         0.746000\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                               params  mean_test_score\n",
            "1  {'n_neighbors': 23, 'p': 2, 'weights': 'distance'}         0.754571\n",
            "3  {'n_neighbors': 23, 'p': 3, 'weights': 'distance'}         0.746000\n",
            "0   {'n_neighbors': 23, 'p': 2, 'weights': 'uniform'}         0.742571\n",
            "2   {'n_neighbors': 23, 'p': 3, 'weights': 'uniform'}         0.735143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "X28TsY0YNSDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPORT VECTOR MACHINE\n",
        "\n",
        "# Define the model and parameter grid\n",
        "model_svm = SVC()\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Train and evaluate the SVC using grid search\n",
        "grid_result_svm = train_and_evaluate_model(model_svm, param_grid_svm, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOCvhDPl5s5H",
        "outputId": "ffe62be5-9ae4-4927-a532-b91cbf553ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Best Accuracy on Training Set:  0.8411428571428571\n",
            "Accuracy on Validation Set with the best model:  0.836\n",
            "Accuracy on Test Set with the best model:  0.8493333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPORT VECTOR MACHINE\n",
        "\n",
        "# Define the model and parameter grid\n",
        "model_svm = SVC()\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 0.5, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
        "}\n",
        "\n",
        "# Train and evaluate the SVC using grid search\n",
        "grid_result_svm = train_and_evaluate_model(model_svm, param_grid_svm, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7waFzHgd9X1I",
        "outputId": "10fd432b-646f-4047-ec07-372a3e5714ad"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Best Accuracy on Training Set:  0.8411428571428571\n",
            "Accuracy on Validation Set with the best model:  0.836\n",
            "Accuracy on Test Set with the best model:  0.8493333333333334\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                               params  mean_test_score\n",
            "0    {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}         0.761714\n",
            "1       {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}         0.689714\n",
            "2      {'C': 0.1, 'gamma': 'scale', 'kernel': 'poly'}         0.229429\n",
            "3   {'C': 0.1, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.801143\n",
            "4     {'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'}         0.761714\n",
            "..                                                ...              ...\n",
            "95     {'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.713429\n",
            "96     {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}         0.758857\n",
            "97        {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}         0.817429\n",
            "98       {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}         0.750857\n",
            "99    {'C': 100, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.749429\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                               params  mean_test_score\n",
            "41        {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}         0.841143\n",
            "81      {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}         0.840286\n",
            "61       {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}         0.840286\n",
            "85       {'C': 100, 'gamma': 'auto', 'kernel': 'rbf'}         0.838857\n",
            "65        {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}         0.838857\n",
            "45         {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}         0.838571\n",
            "21      {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}         0.836000\n",
            "25       {'C': 0.5, 'gamma': 'auto', 'kernel': 'rbf'}         0.835714\n",
            "57          {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}         0.831429\n",
            "37        {'C': 0.5, 'gamma': 0.001, 'kernel': 'rbf'}         0.828857\n",
            "23  {'C': 0.5, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.822857\n",
            "59      {'C': 1, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.822286\n",
            "27   {'C': 0.5, 'gamma': 'auto', 'kernel': 'sigmoid'}         0.821429\n",
            "39    {'C': 0.5, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.819714\n",
            "77         {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}         0.817429\n",
            "97        {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}         0.817429\n",
            "47     {'C': 1, 'gamma': 'auto', 'kernel': 'sigmoid'}         0.816000\n",
            "43    {'C': 1, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.814857\n",
            "7    {'C': 0.1, 'gamma': 'auto', 'kernel': 'sigmoid'}         0.803143\n",
            "3   {'C': 0.1, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.801143\n",
            "15     {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.800000\n",
            "79     {'C': 10, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.789714\n",
            "35     {'C': 0.5, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.766571\n",
            "63   {'C': 10, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.762286\n",
            "0    {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}         0.761714\n",
            "4     {'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'}         0.761714\n",
            "8        {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}         0.761714\n",
            "16     {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}         0.761714\n",
            "12      {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}         0.761714\n",
            "67    {'C': 10, 'gamma': 'auto', 'kernel': 'sigmoid'}         0.760000\n",
            "24    {'C': 0.5, 'gamma': 'auto', 'kernel': 'linear'}         0.758857\n",
            "40     {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}         0.758857\n",
            "56       {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}         0.758857\n",
            "96     {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}         0.758857\n",
            "52        {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}         0.758857\n",
            "64     {'C': 10, 'gamma': 'auto', 'kernel': 'linear'}         0.758857\n",
            "48         {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}         0.758857\n",
            "92      {'C': 100, 'gamma': 0.01, 'kernel': 'linear'}         0.758857\n",
            "44      {'C': 1, 'gamma': 'auto', 'kernel': 'linear'}         0.758857\n",
            "80   {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}         0.758857\n",
            "68        {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}         0.758857\n",
            "88       {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}         0.758857\n",
            "76      {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}         0.758857\n",
            "20   {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}         0.758857\n",
            "28       {'C': 0.5, 'gamma': 0.1, 'kernel': 'linear'}         0.758857\n",
            "84    {'C': 100, 'gamma': 'auto', 'kernel': 'linear'}         0.758857\n",
            "32      {'C': 0.5, 'gamma': 0.01, 'kernel': 'linear'}         0.758857\n",
            "72       {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}         0.758857\n",
            "36     {'C': 0.5, 'gamma': 0.001, 'kernel': 'linear'}         0.758857\n",
            "60    {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}         0.758857\n",
            "94        {'C': 100, 'gamma': 0.01, 'kernel': 'poly'}         0.750857\n",
            "74         {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}         0.750857\n",
            "66       {'C': 10, 'gamma': 'auto', 'kernel': 'poly'}         0.750857\n",
            "70          {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}         0.750857\n",
            "82     {'C': 100, 'gamma': 'scale', 'kernel': 'poly'}         0.750857\n",
            "62      {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}         0.750857\n",
            "86      {'C': 100, 'gamma': 'auto', 'kernel': 'poly'}         0.750857\n",
            "90         {'C': 100, 'gamma': 0.1, 'kernel': 'poly'}         0.750857\n",
            "98       {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}         0.750857\n",
            "50           {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}         0.750857\n",
            "10         {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}         0.750857\n",
            "54          {'C': 1, 'gamma': 0.01, 'kernel': 'poly'}         0.750857\n",
            "30         {'C': 0.5, 'gamma': 0.1, 'kernel': 'poly'}         0.750857\n",
            "14        {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}         0.750857\n",
            "34        {'C': 0.5, 'gamma': 0.01, 'kernel': 'poly'}         0.750857\n",
            "55       {'C': 1, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.750000\n",
            "99    {'C': 100, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.749429\n",
            "83  {'C': 100, 'gamma': 'scale', 'kernel': 'sigmoid'}         0.742571\n",
            "87   {'C': 100, 'gamma': 'auto', 'kernel': 'sigmoid'}         0.734000\n",
            "75      {'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.720000\n",
            "42       {'C': 1, 'gamma': 'scale', 'kernel': 'poly'}         0.717429\n",
            "19    {'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}         0.716857\n",
            "17        {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}         0.715429\n",
            "95     {'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}         0.713429\n",
            "93         {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}         0.710000\n",
            "73          {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}         0.710000\n",
            "46        {'C': 1, 'gamma': 'auto', 'kernel': 'poly'}         0.701143\n",
            "5        {'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}         0.694857\n",
            "53           {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}         0.690286\n",
            "1       {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}         0.689714\n",
            "78        {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}         0.609143\n",
            "22     {'C': 0.5, 'gamma': 'scale', 'kernel': 'poly'}         0.577429\n",
            "26      {'C': 0.5, 'gamma': 'auto', 'kernel': 'poly'}         0.543143\n",
            "11      {'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}         0.391143\n",
            "51        {'C': 1, 'gamma': 0.1, 'kernel': 'sigmoid'}         0.372286\n",
            "31      {'C': 0.5, 'gamma': 0.1, 'kernel': 'sigmoid'}         0.368571\n",
            "71       {'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}         0.363714\n",
            "91      {'C': 100, 'gamma': 0.1, 'kernel': 'sigmoid'}         0.362857\n",
            "2      {'C': 0.1, 'gamma': 'scale', 'kernel': 'poly'}         0.229429\n",
            "6       {'C': 0.1, 'gamma': 'auto', 'kernel': 'poly'}         0.227429\n",
            "69           {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}         0.226000\n",
            "49            {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}         0.226000\n",
            "9           {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}         0.226000\n",
            "29          {'C': 0.5, 'gamma': 0.1, 'kernel': 'rbf'}         0.226000\n",
            "38       {'C': 0.5, 'gamma': 0.001, 'kernel': 'poly'}         0.226000\n",
            "18       {'C': 0.1, 'gamma': 0.001, 'kernel': 'poly'}         0.226000\n",
            "89          {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}         0.226000\n",
            "13         {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}         0.226000\n",
            "33         {'C': 0.5, 'gamma': 0.01, 'kernel': 'rbf'}         0.226000\n",
            "58         {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}         0.226000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplayer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "ObZrbrwHNlKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTIPLAYER PERCEPTRON (MLP) CLASSIFIER\n",
        "\n",
        "# Define the model and parameter grid for grid search\n",
        "model_mlp = MLPClassifier()\n",
        "param_grid_mlp = {\n",
        "    'hidden_layer_sizes': [(100,), (50, 50), (50, 30, 20)], # Architecture of hidden layers\n",
        "    'activation': ['relu', 'tanh'], # Activation function\n",
        "    'solver': ['adam', 'sgd'], # Optimization algorithm for weight updates\n",
        "    'alpha': [0.0001, 0.001, 0.01], # Controls L2 regularization strength\n",
        "}\n",
        "\n",
        "# Train and evaluate the MLP using grid search\n",
        "grid_result_mlp = train_and_evaluate_model(model_mlp, param_grid_mlp, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_mlp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pR3xuveZJvO",
        "outputId": "ba495ce2-1955-4f67-abde-56dfeec06cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'}\n",
            "Best Accuracy on Training Set:  0.812\n",
            "Accuracy on Validation Set with the best model:  0.7826666666666666\n",
            "Accuracy on Test Set with the best model:  0.816\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                               params  mean_test_score\n",
            "0   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.810286\n",
            "1   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.796000\n",
            "2   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.796571\n",
            "3   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.780000\n",
            "4   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.794000\n",
            "5   {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.770571\n",
            "6   {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.808000\n",
            "7   {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.793143\n",
            "8   {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.798286\n",
            "9   {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.784000\n",
            "10  {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.793714\n",
            "11  {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.776286\n",
            "12  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.812000\n",
            "13  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.795143\n",
            "14  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.806571\n",
            "15  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.784286\n",
            "16  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.796000\n",
            "17  {'activation': 'relu', 'alpha': 0.01, 'hidden_...         0.770000\n",
            "18  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.787143\n",
            "19  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.799429\n",
            "20  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.787714\n",
            "21  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.791714\n",
            "22  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.780000\n",
            "23  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...         0.783143\n",
            "24  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.790000\n",
            "25  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.795143\n",
            "26  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.783143\n",
            "27  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.785143\n",
            "28  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.781143\n",
            "29  {'activation': 'tanh', 'alpha': 0.001, 'hidden...         0.768286\n",
            "30  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.794857\n",
            "31  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.797429\n",
            "32  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.784571\n",
            "33  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.799714\n",
            "34  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.780286\n",
            "35  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...         0.779143\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                                                                           params  \\\n",
            "12          {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "0         {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "6          {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "14        {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "33         {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "19         {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "8        {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "31           {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "2       {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "1          {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "16    {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "25          {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "13           {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "30          {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "4   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "10   {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "7           {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'sgd'}   \n",
            "21       {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "24         {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "20      {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "18        {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'adam'}   \n",
            "27        {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "32        {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "15         {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "9         {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "23   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "26       {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}   \n",
            "28   {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "34    {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "22  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'adam'}   \n",
            "3        {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'solver': 'sgd'}   \n",
            "35     {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "11    {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "5    {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "17     {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "29    {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50, 30, 20), 'solver': 'sgd'}   \n",
            "\n",
            "    mean_test_score  \n",
            "12         0.812000  \n",
            "0          0.810286  \n",
            "6          0.808000  \n",
            "14         0.806571  \n",
            "33         0.799714  \n",
            "19         0.799429  \n",
            "8          0.798286  \n",
            "31         0.797429  \n",
            "2          0.796571  \n",
            "1          0.796000  \n",
            "16         0.796000  \n",
            "25         0.795143  \n",
            "13         0.795143  \n",
            "30         0.794857  \n",
            "4          0.794000  \n",
            "10         0.793714  \n",
            "7          0.793143  \n",
            "21         0.791714  \n",
            "24         0.790000  \n",
            "20         0.787714  \n",
            "18         0.787143  \n",
            "27         0.785143  \n",
            "32         0.784571  \n",
            "15         0.784286  \n",
            "9          0.784000  \n",
            "23         0.783143  \n",
            "26         0.783143  \n",
            "28         0.781143  \n",
            "34         0.780286  \n",
            "22         0.780000  \n",
            "3          0.780000  \n",
            "35         0.779143  \n",
            "11         0.776286  \n",
            "5          0.770571  \n",
            "17         0.770000  \n",
            "29         0.768286  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "8P6jNDgoOGCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM FOREST\n",
        "\n",
        "# Define the RandomForestClassifier model and parameter grid\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Train and evaluate the RandomForestClassifier using grid search\n",
        "grid_result_rf = train_and_evaluate_model(model_rf, param_grid_rf, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG2NcBJDqdSX",
        "outputId": "f9ea880b-08f8-491a-d9af-68f968b0b664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bebdb5a2842e>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train and evaluate the RandomForestClassifier using grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgrid_result_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0f0f4ce4549a>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, param_grid, X_train, y_train, X_val, y_val, X_test, y_test)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Step 2: Print the best parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM FOREST\n",
        "\n",
        "# Define the RandomForestClassifier model and parameter grid\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [650],\n",
        "    'max_depth': [40],\n",
        "    'min_samples_split': [10],\n",
        "    'min_samples_leaf': [4],\n",
        "}\n",
        "\n",
        "# Train and evaluate the RandomForestClassifier using grid search\n",
        "grid_result_rf = train_and_evaluate_model(model_rf, param_grid_rf, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDoyyX75rc9j",
        "outputId": "a6c9bc6d-1d6f-44ae-a4cd-538f8ee3e8f6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'max_depth': 40, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 650}\n",
            "Best Accuracy on Training Set:  0.7485714285714286\n",
            "Accuracy on Validation Set with the best model:  0.7373333333333333\n",
            "Accuracy on Test Set with the best model:  0.7626666666666667\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                              params  mean_test_score\n",
            "0  {'max_depth': 40, 'min_samples_leaf': 4, 'min_...         0.748571\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                                                                   params  \\\n",
            "0  {'max_depth': 40, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 650}   \n",
            "\n",
            "   mean_test_score  \n",
            "0         0.748571  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "u8jHC_UVNg6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST\n",
        "\n",
        "# Labels must have integer values starting from 0\n",
        "y_train =  y_train - 1\n",
        "y_val = y_val - 1\n",
        "y_test = y_test - 1\n",
        "\n",
        "# Define the model and parameter grid for grid search\n",
        "model_xgb = XGBClassifier()\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50],\n",
        "    'learning_rate': [0.01],\n",
        "    'max_depth': [5],\n",
        "    'subsample': [1] ,\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Train and evaluate the SVC using grid search\n",
        "grid_result_svm = train_and_evaluate_model(model_xgb, param_grid_xgb, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_svm)\n",
        "\n",
        "# Assign the original values for y\n",
        "y_train =  y_train + 1\n",
        "y_val = y_val + 1\n",
        "y_test = y_test + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "n2kon_WHSLgf",
        "outputId": "228fc721-2a7c-4471-db5a-7d80b4972292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4f4d47d469a5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Labels must have integer values starting from 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0my_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting Classifier"
      ],
      "metadata": {
        "id": "tazjyQW4Nys-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VOTING CLASSIFIER\n",
        "\n",
        "# Define the base models\n",
        "model_svm = SVC(probability=True)\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_mlp = MLPClassifier()\n",
        "\n",
        "# Define the ensemble model\n",
        "#model_vc = VotingClassifier(estimators=[('svm', model_svm), ('rf', model_rf)],\n",
        "#                            voting='soft')\n",
        "model_vc = VotingClassifier(estimators=[('svm', model_svm), ('mlp', model_mlp),\n",
        "                                        ('rf', model_rf)], voting='soft')\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid_vc = {\n",
        "    'svm__C': [0.1, 1, 10],           # SVM regularization parameter\n",
        "    'rf__n_estimators': [200], # Number of trees in the Random Forest\n",
        "    'mlp__hidden_layer_sizes': [(100,)], # Architecture of hidden layers\n",
        "    'mlp__activation': ['relu'], # Activation function\n",
        "    'mlp__solver': ['adam'], # Optimization algorithm for weight updates\n",
        "    'mlp__alpha': [ 0.001, 0.01], # Controls L2 regularization strength\n",
        "}\n",
        "\n",
        "# Train and evaluate the Voting Classifier using grid search\n",
        "grid_result_vc = train_and_evaluate_model(model_vc, param_grid_vc, X_train,\n",
        "                                          y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_vc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL7XkF4C2kL7",
        "outputId": "c12c1908-aef1-42e1-c8be-18b2a4433ef0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 10}\n",
            "Best Accuracy on Training Set:  0.8348571428571429\n",
            "Accuracy on Validation Set with the best model:  0.804\n",
            "Accuracy on Test Set with the best model:  0.8186666666666667\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                              params  mean_test_score\n",
            "0  {'mlp__activation': 'relu', 'mlp__alpha': 0.00...         0.823714\n",
            "1  {'mlp__activation': 'relu', 'mlp__alpha': 0.00...         0.829143\n",
            "2  {'mlp__activation': 'relu', 'mlp__alpha': 0.00...         0.830571\n",
            "3  {'mlp__activation': 'relu', 'mlp__alpha': 0.01...         0.829143\n",
            "4  {'mlp__activation': 'relu', 'mlp__alpha': 0.01...         0.825143\n",
            "5  {'mlp__activation': 'relu', 'mlp__alpha': 0.01...         0.834857\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                                                                                                                               params  \\\n",
            "5    {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 10}   \n",
            "2   {'mlp__activation': 'relu', 'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 10}   \n",
            "1    {'mlp__activation': 'relu', 'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 1}   \n",
            "3   {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 0.1}   \n",
            "4     {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 1}   \n",
            "0  {'mlp__activation': 'relu', 'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': (100,), 'mlp__solver': 'adam', 'rf__n_estimators': 200, 'svm__C': 0.1}   \n",
            "\n",
            "   mean_test_score  \n",
            "5         0.834857  \n",
            "2         0.830571  \n",
            "1         0.829143  \n",
            "3         0.829143  \n",
            "4         0.825143  \n",
            "0         0.823714  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking\n"
      ],
      "metadata": {
        "id": "-6eb3XAoxzer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STACKING\n",
        "# Define the base models\n",
        "svm = SVC(C=0.8, gamma='scale', kernel='rbf', probability=True)\n",
        "model_rf = RandomForestClassifier(max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=650)\n",
        "model_mlp = MLPClassifier(activation='relu', alpha=0.01, hidden_layer_sizes=(100,), solver='adam')\n",
        "model_knn = KNeighborsClassifier(n_neighbors=23, p=2, weights='distance')\n",
        "\n",
        "# Define the stacking model with Logistic Regression as the meta-estimator\n",
        "model_stacking = StackingClassifier(estimators=[('svm', model_svm), ('rf', model_rf),\n",
        "                                                ('knn', model_knn), ('mlp', model_mlp)],\n",
        "                                    final_estimator=model_rf)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid_stacking = {\n",
        "    'svm__C': [1],           # SVM regularization parameter\n",
        "    #'rf__n_estimators': [100, 200],           # Number of trees in the Random Forest\n",
        "    #'mlp__hidden_layer_sizes': [(100,)],          # Architecture of hidden layers\n",
        "    #'mlp__activation': ['relu'],                  # Activation function\n",
        "    #'mlp__solver': ['adam'],                      # Optimization algorithm for weight updates\n",
        "    #'mlp__alpha': [0.0001, 0.001, 0.01],          # Controls L2 regularization strength\n",
        "}\n",
        "\n",
        "# Train and evaluate the Stacking Classifier using grid search\n",
        "grid_result_stacking = train_and_evaluate_model(model_stacking, param_grid_stacking, X_train,\n",
        "                                                y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_stacking)"
      ],
      "metadata": {
        "id": "1WgAcqEbI01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb681bc4-09f6-406e-df77-4b0c3b919ac1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'svm__C': 1}\n",
            "Best Accuracy on Training Set:  0.8571428571428571\n",
            "Accuracy on Validation Set with the best model:  0.844\n",
            "Accuracy on Test Set with the best model:  0.8613333333333333\n",
            "\n",
            "All Scores from Grid Search:\n",
            "          params  mean_test_score\n",
            "0  {'svm__C': 1}         0.857143\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "          params  mean_test_score\n",
            "0  {'svm__C': 1}         0.857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagged Desicion Trees"
      ],
      "metadata": {
        "id": "1k0WOD5D44BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BAGGED DECISION TREES\n",
        "\n",
        "# Define the base Decision Tree model\n",
        "base_model = DecisionTreeClassifier()\n",
        "\n",
        "# Define the BaggingClassifier model\n",
        "bagged_model = BaggingClassifier(base_model, n_estimators=200, random_state=42)\n",
        "\n",
        "# Train the BaggingClassifier model on the training data\n",
        "bagged_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_bagged = bagged_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test data\n",
        "accuracy_bagged = accuracy_score(y_test, y_pred_bagged)\n",
        "\n",
        "# Display results\n",
        "print(\"Accuracy of Bagged Decision Trees on the test dataset:\", accuracy_bagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfBkvkGN405x",
        "outputId": "29521c54-8050-462b-a867-3f72aafd574a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Bagged Decision Trees on the test dataset: 0.688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ada Boost"
      ],
      "metadata": {
        "id": "iJ5hkUcH05T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADA BOOST\n",
        "\n",
        "# Define the AdaBoostClassifier model and parameter grid\n",
        "base_model = DecisionTreeClassifier()\n",
        "model_adaboost = AdaBoostClassifier(base_model, random_state=42)\n",
        "param_grid_adaboost = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "}\n",
        "\n",
        "# Train and evaluate the AdaBoostClassifier using grid search\n",
        "grid_result_adaboost = train_and_evaluate_model(model_adaboost, param_grid_adaboost, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Display results\n",
        "display_grid_search_results(grid_result_adaboost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn-dZsuf0wE5",
        "outputId": "a8e5d2b2-0a95-404c-ab76-c437ab14e9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'learning_rate': 0.01, 'n_estimators': 50}\n",
            "Best Accuracy on Training Set:  0.3562857142857143\n",
            "Accuracy on Validation Set with the best model:  0.364\n",
            "Accuracy on Test Set with the best model:  0.3333333333333333\n",
            "\n",
            "All Scores from Grid Search:\n",
            "                                         params  mean_test_score\n",
            "0   {'learning_rate': 0.01, 'n_estimators': 50}         0.356286\n",
            "1  {'learning_rate': 0.01, 'n_estimators': 100}         0.356286\n",
            "2  {'learning_rate': 0.01, 'n_estimators': 200}         0.356286\n",
            "3    {'learning_rate': 0.1, 'n_estimators': 50}         0.356286\n",
            "4   {'learning_rate': 0.1, 'n_estimators': 100}         0.356286\n",
            "5   {'learning_rate': 0.1, 'n_estimators': 200}         0.356286\n",
            "6      {'learning_rate': 1, 'n_estimators': 50}         0.356286\n",
            "7     {'learning_rate': 1, 'n_estimators': 100}         0.356286\n",
            "8     {'learning_rate': 1, 'n_estimators': 200}         0.356286\n",
            "\n",
            "Hyperparameters sorted by Mean Test Score in Descending Order:\n",
            "                                         params  mean_test_score\n",
            "0   {'learning_rate': 0.01, 'n_estimators': 50}         0.356286\n",
            "1  {'learning_rate': 0.01, 'n_estimators': 100}         0.356286\n",
            "2  {'learning_rate': 0.01, 'n_estimators': 200}         0.356286\n",
            "3    {'learning_rate': 0.1, 'n_estimators': 50}         0.356286\n",
            "4   {'learning_rate': 0.1, 'n_estimators': 100}         0.356286\n",
            "5   {'learning_rate': 0.1, 'n_estimators': 200}         0.356286\n",
            "6      {'learning_rate': 1, 'n_estimators': 50}         0.356286\n",
            "7     {'learning_rate': 1, 'n_estimators': 100}         0.356286\n",
            "8     {'learning_rate': 1, 'n_estimators': 200}         0.356286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final model"
      ],
      "metadata": {
        "id": "aCcZ5k08A0XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL PREDICTION\n",
        "\n",
        "# Instantiate each model with its best parameters\n",
        "random_forest = RandomForestClassifier(max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=650)\n",
        "svm = SVC(C=0.8, gamma='scale', kernel='rbf', probability=True)\n",
        "mlp = MLPClassifier(activation='relu', alpha=0.01, hidden_layer_sizes=(100,), solver='adam')\n",
        "naive_bayes = GaussianNB()\n",
        "xgboost = XGBClassifier(colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8)\n",
        "model_knn = KNeighborsClassifier(n_neighbors=23, p=2, weights='distance')\n",
        "model_lr = LogisticRegression(C=0.01)\n",
        "\n",
        "# Create the ensemble model with soft voting\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('Random Forest', random_forest),\n",
        "    ('SVM', svm),\n",
        "    #('MLP', mlp),\n",
        "    #('nb', naive_bayes),\n",
        "    #('XGBoost', xgboost),\n",
        "    #('LogReg', model_lr),\n",
        "    ('Knn', model_knn)\n",
        "], voting='soft')\n",
        "\n",
        "# Create a stacking model with same mean error as ensemble model with soft voting\n",
        "model_stacking = StackingClassifier(estimators=[('svm', model_svm), ('rf', model_rf),\n",
        "                                                ('knn', model_knn), ('mlp', model_mlp)],\n",
        "                                    final_estimator=model_rf)\n",
        "\n",
        "# Train the model on the training data\n",
        "model_stacking.fit(X_train, y_train -1)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_stacking.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred +1)\n",
        "\n",
        "# Display results\n",
        "print(\"Accuracy of Final model on the test dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hET7MAtAIBAG",
        "outputId": "a06d8b57-ad7e-410e-a487-d9df48325dc5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Final model on the test dataset: 0.8586666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training data\n",
        "model_stacking.fit(X, y - 1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = model_stacking.predict(final_x_test)\n",
        "\n",
        "# Print shape of y_test_pred\n",
        "print(y_test_pred.shape)\n",
        "\n",
        "# Convert to numpy Array and export the label\n",
        "y_test_pred_np = np.array(y_test_pred +1)\n",
        "np.save('labels22.npy', y_test_pred_np)\n",
        "\n",
        "# Load the saved labels\n",
        "loaded_labels = np.load('labels22.npy')\n",
        "\n",
        "# Display the loaded labels\n",
        "print(\"Loaded Labels:\")\n",
        "print(loaded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wxStKXiOcl8",
        "outputId": "e34692ba-13be-4717-b02d-559aebf7a521"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000,)\n",
            "Loaded Labels:\n",
            "[5 2 4 1 4 1 2 5 3 4 3 5 1 2 1 5 5 3 5 1 5 5 5 1 1 2 2 3 2 2 2 2 2 1 2 2 3\n",
            " 5 3 3 1 2 2 5 4 2 1 5 2 4 3 2 1 2 1 3 2 1 2 4 1 1 3 2 1 3 3 1 3 4 1 4 5 1\n",
            " 1 2 4 3 5 1 1 5 4 4 1 5 5 4 5 2 4 2 3 3 2 4 5 2 4 3 4 1 1 4 5 5 2 4 3 2 5\n",
            " 5 5 4 5 3 1 5 3 4 2 1 1 1 4 5 3 2 5 2 1 5 5 1 4 1 2 1 4 3 4 4 2 4 3 5 2 1\n",
            " 4 2 3 1 2 3 3 3 4 3 4 1 1 1 2 3 4 4 5 2 3 1 1 1 2 2 3 5 4 5 1 1 5 5 3 3 4\n",
            " 3 4 5 1 3 5 3 2 3 3 2 1 2 3 1 3 1 2 1 3 3 5 2 3 5 4 5 2 4 5 1 1 5 3 3 1 1\n",
            " 4 4 4 2 5 4 4 1 3 2 2 2 3 4 4 5 3 4 2 5 1 2 4 5 1 5 3 2 1 1 3 4 4 2 3 2 2\n",
            " 5 1 5 1 5 1 5 4 2 1 1 4 4 1 1 5 2 1 3 2 4 2 5 1 1 2 2 2 5 4 5 4 3 1 2 2 3\n",
            " 3 4 3 5 4 1 1 4 2 5 1 4 1 2 1 4 4 3 5 1 4 4 4 4 5 3 5 3 3 2 1 5 1 5 3 1 3\n",
            " 1 4 2 1 1 3 3 3 4 2 5 1 1 2 2 4 1 4 4 3 2 5 5 4 4 1 1 2 1 2 4 3 3 3 2 4 2\n",
            " 5 1 3 3 5 5 5 4 3 4 3 5 5 3 2 4 4 3 2 5 4 2 5 1 2 3 4 5 3 5 2 1 5 2 3 4 5\n",
            " 4 5 5 1 1 2 3 3 4 3 5 4 1 1 1 4 2 3 4 3 4 4 1 3 1 4 4 5 3 3 5 1 3 3 4 1 1\n",
            " 3 4 5 1 5 3 1 2 4 3 3 5 5 1 4 4 1 4 2 3 1 4 1 4 2 1 4 5 2 4 5 2 1 2 4 3 5\n",
            " 2 5 2 4 5 2 4 3 4 1 2 2 2 1 3 1 5 2 4 3 3 5 4 3 2 3 1 4 2 4 4 3 3 3 2 4 2\n",
            " 1 3 4 4 5 3 3 2 2 3 3 5 4 3 1 4 4 1 1 1 3 5 1 1 1 5 2 1 4 2 1 2 1 3 2 2 1\n",
            " 5 2 3 2 2 1 2 2 3 1 4 3 1 5 4 5 4 4 1 1 3 3 5 1 4 4 2 4 1 4 5 3 1 1 2 4 4\n",
            " 3 3 1 5 5 4 2 3 3 1 3 2 1 4 1 3 3 3 4 1 1 2 3 3 2 1 5 5 1 2 3 1 1 2 2 4 4\n",
            " 2 3 5 2 4 5 1 5 2 1 1 1 1 5 3 3 2 1 2 2 3 1 2 2 3 4 3 3 1 5 1 4 1 2 3 4 5\n",
            " 5 1 1 3 2 3 1 4 1 5 2 1 5 1 5 1 1 2 4 1 5 2 4 2 4 2 1 4 3 5 2 1 5 3 5 1 1\n",
            " 4 5 1 4 5 1 4 2 4 1 1 4 2 2 1 1 5 2 1 2 4 2 3 5 2 4 4 1 3 5 5 4 5 2 3 5 3\n",
            " 3 5 5 3 2 5 2 4 4 4 5 1 2 1 5 5 1 1 3 4 2 1 2 3 2 1 5 3 2 1 3 1 3 3 1 3 1\n",
            " 4 1 2 3 3 1 2 1 2 1 4 2 3 5 1 2 1 2 2 4 2 5 5 1 5 5 2 1 1 1 1 5 3 1 2 1 4\n",
            " 4 4 1 5 1 1 1 5 4 5 2 2 4 3 4 3 4 4 2 5 5 5 1 3 1 2 4 1 5 1 1 1 4 4 4 2 5\n",
            " 5 2 4 2 5 5 3 4 3 4 4 3 1 2 2 2 5 2 1 1 2 1 5 4 1 4 2 2 3 3 3 1 4 1 1 4 2\n",
            " 2 5 1 3 3 4 5 3 5 1 2 3 4 5 5 3 5 1 2 1 3 4 5 4 4 2 1 3 5 2 5 2 5 2 1 2 1\n",
            " 4 3 5 2 1 1 2 5 3 5 4 5 1 4 5 4 1 2 2 1 3 4 4 1 5 5 5 3 4 5 3 1 1 2 3 1 1\n",
            " 4 4 3 3 2 4 5 4 5 5 5 2 1 2 3 3 5 4 1 5 4 4 1 2 3 4 5 5 1 2 5 1 4 5 3 5 4\n",
            " 1]\n"
          ]
        }
      ]
    }
  ]
}