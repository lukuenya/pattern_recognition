{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass classification using a neural network\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as c\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('./dataset/datasetC.csv', header=None)\n",
    "df_testing = pd.read_csv('./dataset/datasetCTest.csv', header=None)\n",
    "\n",
    "# X is all the data except the last column\n",
    "X_training = df_training.iloc[:, :-1]\n",
    "\n",
    "# y is the last column\n",
    "y_training = df_training.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of models\n",
    "models = {  \n",
    "    \"xgboost\": OneVsRestClassifier(XGBClassifier(random_state=42)),\n",
    "    \"SGD\" : SGDClassifier(random_state=42),\n",
    "    \"svc\" : OneVsRestClassifier(SVC()),\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#y_training = y_training - 1\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the scaler for later use\n",
    "import pickle\n",
    "pickle.dump(scaler, open(\"myscaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost\n",
      "- Accuracy : 0.7970\n",
      "\n",
      "\n",
      "SGD\n",
      "- Accuracy : 0.7590\n",
      "\n",
      "\n",
      "svc\n",
      "- Accuracy : 0.8330\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "for i in range(len(list(models))):\n",
    "  model = list(models.values())[i]\n",
    "  model.fit(x_train_scaled, y_train)\n",
    "\n",
    "  #Make predictions\n",
    "  y_test_pred = model.predict(x_test_scaled)\n",
    "\n",
    "  # Test set Performance\n",
    "  accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "  print(list(models.keys())[i])\n",
    "\n",
    "  print('- Accuracy : {:.4f}'.format(accuracy))\n",
    "\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 6s 6ms/step - loss: 2.1240 - accuracy: 0.2528 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 1.5178 - accuracy: 0.4055 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 1.2785 - accuracy: 0.4935 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.1296 - accuracy: 0.5807 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0366 - accuracy: 0.6225 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.9710 - accuracy: 0.6492 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.9176 - accuracy: 0.6747 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8914 - accuracy: 0.6770 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8462 - accuracy: 0.7113 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8045 - accuracy: 0.7197 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.7607 - accuracy: 0.7303 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7621 - accuracy: 0.7437 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7326 - accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.7354 - accuracy: 0.7475 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.7238 - accuracy: 0.7533 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.7710 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.7663 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "394/400 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.7675\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6753 - accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6643 - accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6574 - accuracy: 0.7725 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.6178 - accuracy: 0.7903 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6412 - accuracy: 0.7810 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.6151 - accuracy: 0.7958 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6080 - accuracy: 0.7945 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6178 - accuracy: 0.7980 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "390/400 [============================>.] - ETA: 0s - loss: 0.6210 - accuracy: 0.7869\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6187 - accuracy: 0.7875 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.8142 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.8090 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5772 - accuracy: 0.8040 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5602 - accuracy: 0.8092 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5439 - accuracy: 0.8152 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5734 - accuracy: 0.8055 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "396/400 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.8088\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5731 - accuracy: 0.8090 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5630 - accuracy: 0.8108 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.5498 - accuracy: 0.8167\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5498 - accuracy: 0.8167 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5780 - accuracy: 0.8142 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.8203\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5613 - accuracy: 0.8202 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5659 - accuracy: 0.8175 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "394/400 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.8195\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5613 - accuracy: 0.8190 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.5547 - accuracy: 0.8152 - lr: 1.5625e-05\n",
      "Epoch 41/100\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.5530 - accuracy: 0.8163\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.5528 - accuracy: 0.8165 - lr: 1.5625e-05\n",
      "32/32 [==============================] - 1s 4ms/step - loss: 0.5489 - accuracy: 0.8110\n",
      "Test accuracy: 0.8109999895095825\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "\n",
    "# Create a Sequential model\n",
    "model_nn = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 20 neurons\n",
    "model_nn.add(Dense(30, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model_nn.add(BatchNormalization())\n",
    "\n",
    "# model_nn.add(Dense(32, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "# model_nn.add(Dense(6, activation='softmax'))  # 6 neurons in the output layer\n",
    "\n",
    "# Add dropout after the first hidden layer\n",
    "model_nn.add(Dropout(0.5))\n",
    "\n",
    "model_nn.add(Dense(20, activation='relu'))\n",
    "model_nn.add(BatchNormalization())\n",
    "\n",
    "# Add dropout after the second hidden layer\n",
    "model_nn.add(Dropout(0.5))\n",
    "\n",
    "# Add an output layer with a neuron for each class, using softmax activation\n",
    "model_nn.add(Dense(y_train_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "# Define learning rate reduction\n",
    "lr_reduction = ReduceLROnPlateau(monitor='loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "model_nn.fit(x_train_scaled, y_train_categorical, epochs=100, batch_size=10, callbacks=[early_stopping, lr_reduction])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model_nn.evaluate(x_test_scaled, to_categorical(y_test))\n",
    "\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_nn.save('model_nn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model('model_nn.keras')\n",
    "\n",
    "# New data = df_testing\n",
    "x_test_new = df_testing\n",
    "\n",
    "# Scaling the data\n",
    "scaler = pickle.load(open(\"myscaler.pkl\", \"rb\"))\n",
    "x_test_new_scaled = scaler.transform(x_test_new)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(x_test_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to label indexes using argmax\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Add 1 to the predicted labels to get them back in the original format\n",
    "predicted_labels_original = predicted_labels + 1\n",
    "\n",
    "# Plot the distribution of predicted classes\n",
    "plt.hist(predicted_labels_original, bins=np.arange(predictions.shape[1] + 1) - 0.5, edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of predicted classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted labels to a numpy file\n",
    "np.save('labels22.npy', predicted_labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predicted labels\n",
    "loaded_labels = np.load('labels22.npy')\n",
    "\n",
    "# Print the first 20 entries of the predicted labels\n",
    "print(loaded_labels[:400])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
